import { sendAI } from "@messages/api/sendAI"
import { getModels } from "@utils/config/runtimeConfig"
import { SWEBenchLoader } from "./SWEBenchLoader"

// run in the terminal with:
// bun run src/core/workflow/ingestion/benchmarks/swe/test_swe.ts

// Common SWE-bench instance IDs for random selection
const SAMPLE_INSTANCE_IDS = [
  "django__django-11099",
  "requests__requests-2317",
  "django__django-12453",
  "requests__requests-3179",
  "django__django-13028",
  "requests__requests-2674",
  "django__django-11630",
  "requests__requests-3718",
  "django__django-12700",
  "requests__requests-2918",
  "django__django-11742",
  "requests__requests-3376",
  "django__django-13230",
  "requests__requests-2835",
  "django__django-12747",
]

// Extended SWE-bench instance interface based on the complete schema
interface SWEBenchInstanceFull {
  instance_id: string // A formatted instance identifier, usually as repo_owner__repo_name-PR-number
  patch: string // The gold patch, the patch generated by the PR (minus test-related code), that resolved the issue
  repo: string // The repository owner/name identifier from GitHub
  base_commit: string // The commit hash of the repository representing the HEAD of the repository before the solution PR is applied
  hints_text?: string // Comments made on the issue prior to the creation of the solution PR's first commit creation date
  created_at?: string // The creation date of the pull request
  test_patch?: string // A test-file patch that was contributed by the solution PR
  problem_statement: string // The issue title and body
  version?: string // Installation version to use for running evaluation
  environment_setup_commit?: string // commit hash to use for environment setup and installation
  FAIL_TO_PASS?: string[] // A json list of strings that represent the set of tests resolved by the PR and tied to the issue resolution
  PASS_TO_PASS?: string[] // A json list of strings that represent tests that should pass before and after the PR application
}

async function getRandomSWEBenchInstance(): Promise<SWEBenchInstanceFull> {
  const randomId =
    SAMPLE_INSTANCE_IDS[Math.floor(Math.random() * SAMPLE_INSTANCE_IDS.length)]
  console.log(`üé≤ Selected random instance: ${randomId}`)

  try {
    return (await SWEBenchLoader.fetchById(randomId)) as SWEBenchInstanceFull
  } catch (error) {
    console.log(`‚ö†Ô∏è  Could not fetch ${randomId}, trying another...`)
    // Try a different random ID
    const fallbackId =
      SAMPLE_INSTANCE_IDS[
        Math.floor(Math.random() * SAMPLE_INSTANCE_IDS.length)
      ]
    return (await SWEBenchLoader.fetchById(fallbackId)) as SWEBenchInstanceFull
  }
}

async function validateAISolution(
  aiSolution: string,
  instance: SWEBenchInstanceFull
): Promise<{
  overallScore: number
  validationResults: {
    hasCodeChanges: boolean
    addressesProblem: boolean
    formatQuality: boolean
    testAwareness: boolean
    groundTruthSimilarity: number
  }
  feedback: string
  cost: number
}> {
  console.log("\nüîç Validating AI solution against ground truth...")
  console.log("=".repeat(60))

  const validationPrompt = `You are an expert code reviewer evaluating an AI-generated solution for a software engineering problem.

**Original Problem:**
${instance.problem_statement}

**Repository:** ${instance.repo}
**Base Commit:** ${instance.base_commit}

**Ground Truth Solution (Gold Patch):**
${instance.patch}

**AI Generated Solution:**
${aiSolution}

**Additional Context:**
${instance.hints_text ? `- Issue Hints: ${instance.hints_text}` : ""}
${instance.FAIL_TO_PASS ? `- Tests that should pass after fix: ${JSON.stringify(instance.FAIL_TO_PASS)}` : ""}
${instance.PASS_TO_PASS ? `- Tests that should continue passing: ${JSON.stringify(instance.PASS_TO_PASS)}` : ""}

**Your Task:**
Evaluate the AI solution against the ground truth and provide a detailed analysis.

**Evaluation Criteria:**
1. **Code Changes** (0-10): Does the AI solution contain actual code changes/patches?
2. **Problem Addressing** (0-10): Does the AI solution directly address the problem statement?
3. **Format Quality** (0-10): Is the solution well-formatted and clear?
4. **Test Awareness** (0-10): Does the AI solution mention or consider relevant tests?
5. **Ground Truth Similarity** (0-10): How similar is the AI approach to the ground truth solution?

**Response Format:**
Provide scores (0-10) for each criterion and explain your reasoning. Also provide an overall assessment of solution quality.`

  try {
    const response = await sendAI({
      messages: [
        {
          role: "system",
          content:
            "You are an expert code reviewer and software engineering evaluator.",
        },
        { role: "user", content: validationPrompt },
      ],
      model: getModels().default,
      mode: "text",
      opts: {
        reasoning: true,
      },
    })

    if (!response.success || !response.data) {
      throw new Error(response.error || "Validation failed")
    }

    // Simple validation scoring based on content analysis
    const validationResults = {
      hasCodeChanges: /```|diff|patch|\+\+\+|---|\+[^+]|-[^-]|@@ /.test(
        aiSolution
      ),
      addressesProblem: aiSolution
        .toLowerCase()
        .includes(
          instance.problem_statement
            .toLowerCase()
            .split(" ")
            .slice(0, 5)
            .join(" ")
            .toLowerCase()
        ),
      formatQuality:
        aiSolution.length > 100 && /analysis|solution|code/i.test(aiSolution),
      testAwareness: /test|spec|assert|expect|should|verify/i.test(aiSolution),
      groundTruthSimilarity: calculateSimilarity(aiSolution, instance.patch),
    }

    // Calculate overall score
    const scores = {
      hasCodeChanges: validationResults.hasCodeChanges ? 8 : 3,
      addressesProblem: validationResults.addressesProblem ? 8 : 4,
      formatQuality: validationResults.formatQuality ? 7 : 2,
      testAwareness: validationResults.testAwareness ? 6 : 2,
      groundTruthSimilarity: validationResults.groundTruthSimilarity * 10,
    }

    const overallScore =
      Object.values(scores).reduce((a, b) => a + b, 0) /
      Object.keys(scores).length

    console.log("‚úÖ AI Validation Results:")
    console.log(
      "üí∞ Validation Cost:",
      `$${response.usdCost?.toFixed(4) || "0.0000"}`
    )
    console.log("\nüìä Validation Scores:")
    console.log(
      `- Has Code Changes: ${scores.hasCodeChanges}/10 (${validationResults.hasCodeChanges ? "‚úÖ" : "‚ùå"})`
    )
    console.log(
      `- Addresses Problem: ${scores.addressesProblem}/10 (${validationResults.addressesProblem ? "‚úÖ" : "‚ùå"})`
    )
    console.log(
      `- Format Quality: ${scores.formatQuality}/10 (${validationResults.formatQuality ? "‚úÖ" : "‚ùå"})`
    )
    console.log(
      `- Test Awareness: ${scores.testAwareness}/10 (${validationResults.testAwareness ? "‚úÖ" : "‚ùå"})`
    )
    console.log(
      `- Ground Truth Similarity: ${scores.groundTruthSimilarity.toFixed(1)}/10`
    )
    console.log(`\nüéØ Overall Score: ${overallScore.toFixed(1)}/10`)

    console.log("\nü§ñ AI Reviewer Feedback:")
    console.log("-".repeat(50))
    console.log(response.data.text)

    return {
      overallScore,
      validationResults: {
        ...validationResults,
        groundTruthSimilarity: validationResults.groundTruthSimilarity,
      },
      feedback: response.data.text,
      cost: response.usdCost || 0,
    }
  } catch (error) {
    console.error("‚ùå Validation failed:", error)
    throw error
  }
}

// Simple similarity calculation (Jaccard similarity for words)
function calculateSimilarity(text1: string, text2: string): number {
  const words1 = new Set(text1.toLowerCase().match(/\w+/g) || [])
  const words2 = new Set(text2.toLowerCase().match(/\w+/g) || [])

  const intersection = new Set([...words1].filter((x) => words2.has(x)))
  const union = new Set([...words1, ...words2])

  return union.size > 0 ? intersection.size / union.size : 0
}

async function solveSWEBenchInstance(instance: SWEBenchInstanceFull) {
  console.log("\nü§ñ Attempting to solve SWE-bench instance with AI...")
  console.log("=".repeat(60))

  const systemPrompt = `You are an expert software engineer skilled at fixing bugs and implementing features in Python codebases. 

You will be given a problem statement from the SWE-bench dataset which contains real issues from popular open-source repositories.

Your task is to:
1. Understand the problem described in the problem statement
2. Analyze the repository and codebase context
3. Generate a code solution that fixes the issue
4. Provide the solution as a git patch format

Be concise but thorough in your analysis. Focus on the core issue and provide a practical solution.`

  const userPrompt = `Repository: ${instance.repo}
Base Commit: ${instance.base_commit}

Problem Statement:
${instance.problem_statement}

${
  instance.hints_text
    ? `Issue Hints/Comments:
${instance.hints_text}`
    : ""
}

${
  instance.FAIL_TO_PASS
    ? `Tests that should pass after fix:
${JSON.stringify(instance.FAIL_TO_PASS, null, 2)}`
    : ""
}

${
  instance.PASS_TO_PASS
    ? `Tests that should continue passing:
${JSON.stringify(instance.PASS_TO_PASS, null, 2)}`
    : ""
}

Please analyze this issue and provide a solution. Format your response as follows:

1. **Analysis**: Brief explanation of what the issue is
2. **Solution**: Describe your approach to fix it
3. **Code**: Provide the specific code changes needed (in patch format if possible)

Focus on providing a practical, working solution that addresses the failing tests.`

  try {
    const response = await sendAI({
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      model: getModels().default,
      mode: "text",
      opts: {
        reasoning: true,
      },
    })

    if (response.success && response.data) {
      console.log("‚úÖ AI Solution Generated!")
      console.log("üí∞ Cost:", `$${response.usdCost?.toFixed(4) || "0.0000"}`)
      console.log("\nüìù AI Response:")
      console.log("-".repeat(50))
      console.log(response.data.text)

      if (response.data.reasoning) {
        console.log("\nüß† AI Reasoning:")
        console.log("-".repeat(50))
        console.log(response.data.reasoning)
      }
    } else {
      console.log("‚ùå AI call failed:", response.error)
      throw new Error(response.error || "AI call failed")
    }

    return response
  } catch (error) {
    console.error("‚ùå Error during AI solution generation:", error)
    throw error
  }
}

async function testSWEBenchAISolver() {
  console.log("üî¨ Testing SWE-bench AI Solver with Validation")
  console.log("=".repeat(50))

  try {
    // Step 1: Get a random SWE-bench instance
    console.log("\nüìö Fetching random SWE-bench instance...")
    const instance = await getRandomSWEBenchInstance()

    console.log("\nüìã Instance Details:")
    console.log("ID:", instance.instance_id)
    console.log("Repository:", instance.repo)
    console.log("Base commit:", instance.base_commit)
    console.log(
      "Problem statement preview:",
      instance.problem_statement.substring(0, 150) + "..."
    )
    console.log("Has patch:", !!instance.patch)
    console.log("Patch length:", instance.patch?.length || 0)
    console.log("Has test patch:", !!instance.test_patch)
    console.log("Has hints:", !!instance.hints_text)
    console.log("FAIL_TO_PASS tests:", instance.FAIL_TO_PASS?.length || 0)
    console.log("PASS_TO_PASS tests:", instance.PASS_TO_PASS?.length || 0)
    console.log("Created at:", instance.created_at || "Unknown")
    console.log("Version:", instance.version || "Unknown")

    // Step 2: Ask AI to solve the instance
    const aiResponse = await solveSWEBenchInstance(instance)

    // Step 3: Validate the AI solution
    const validation = await validateAISolution(
      aiResponse.data?.text || "",
      instance
    )

    // Step 4: Show ground truth for comparison
    if (instance.patch) {
      console.log("\nüéØ Ground Truth Solution:")
      console.log("-".repeat(50))
      console.log(instance.patch.substring(0, 500) + "...")

      if (instance.test_patch) {
        console.log("\nüß™ Ground Truth Test Patch:")
        console.log("-".repeat(50))
        console.log(instance.test_patch.substring(0, 300) + "...")
      }
    }

    console.log("\nüéâ SWE-bench AI Solver test completed successfully!")
    console.log("=".repeat(50))

    const totalCost = (aiResponse.usdCost || 0) + validation.cost

    return {
      instanceId: instance.instance_id,
      aiSolution: aiResponse.data?.text,
      groundTruth: instance.patch,
      validation: validation,
      cost: totalCost,
    }
  } catch (error) {
    console.error("‚ùå Test failed:", error)
    throw error
  }
}

// Test basic SWEBenchLoader functionality
async function testBasicSWEBenchLoader() {
  console.log("üß™ Testing basic SWEBenchLoader functionality...")

  try {
    const result = await SWEBenchLoader.fetchById("django__django-11099")
    console.log("‚úÖ Basic loader test passed!")
    console.log("Found instance:", result.instance_id)
    console.log("Repository:", result.repo)

    return result
  } catch (error) {
    console.log(
      "‚ö†Ô∏è  Basic loader test failed, but continuing with AI solver test..."
    )
    console.log("Error:", error)
  }
}

// Main test function
async function runTests() {
  console.log("üöÄ Starting SWE-bench AI Solver Tests with Validation")
  console.log("=".repeat(65))

  try {
    // Test 1: Basic SWEBenchLoader
    await testBasicSWEBenchLoader()

    // Test 2: AI Solver with Validation
    const result = await testSWEBenchAISolver()

    console.log("\nüìä Final Results Summary:")
    console.log("=".repeat(40))
    console.log("Instance ID:", result.instanceId)
    console.log(
      "Validation Score:",
      `${result.validation.overallScore.toFixed(1)}/10`
    )
    console.log(
      "Has Code Changes:",
      result.validation.validationResults.hasCodeChanges ? "‚úÖ" : "‚ùå"
    )
    console.log(
      "Addresses Problem:",
      result.validation.validationResults.addressesProblem ? "‚úÖ" : "‚ùå"
    )
    console.log(
      "Format Quality:",
      result.validation.validationResults.formatQuality ? "‚úÖ" : "‚ùå"
    )
    console.log(
      "Test Awareness:",
      result.validation.validationResults.testAwareness ? "‚úÖ" : "‚ùå"
    )
    console.log(
      "Ground Truth Similarity:",
      `${(result.validation.validationResults.groundTruthSimilarity * 10).toFixed(1)}/10`
    )
    console.log("Total Cost:", `$${result.cost.toFixed(4)}`)

    console.log("\n‚úÖ All tests completed successfully!")
  } catch (error) {
    console.error("‚ùå Tests failed:", error)
    process.exit(1)
  }
}

// Run the tests
runTests()
