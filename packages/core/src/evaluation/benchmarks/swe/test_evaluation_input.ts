import { mkdir, readFile, writeFile } from "node:fs/promises"
import { join } from "node:path"
import { PATHS, getDefaultModels } from "@core/core-config/compat"
import { sendAI } from "@core/messages/api/sendAI/sendAI"
import { IngestionLayer } from "@core/workflow/ingestion/IngestionLayer"
import type { EvaluationInput } from "@core/workflow/ingestion/ingestion.types"
import { listFiles } from "@huggingface/hub"
import { z } from "zod"
import { SWEBenchLoader } from "./SWEBenchLoader"

// run in the terminal with:
// bun run src/core/workflow/ingestion/benchmarks/swe/test_evaluation_input.ts

// Extended SWE-bench instance interface based on the complete schema
interface SWEBenchInstanceFull {
  instance_id: string // A formatted instance identifier, usually as repo_owner__repo_name-PR-number
  patch: string // The gold patch, the patch generated by the PR (minus test-related code), that resolved the issue
  repo: string // The repository owner/name identifier from GitHub
  base_commit: string // The commit hash of the repository representing the HEAD of the repository before the solution PR is applied
  hints_text?: string // Comments made on the issue prior to the creation of the solution PR's first commit creation date
  created_at?: string // The creation date of the pull request
  test_patch?: string // A test-file patch that was contributed by the solution PR
  problem_statement: string // The issue title and body
  text: string // Full issue text
  version?: string // Installation version to use for running evaluation
  environment_setup_commit?: string // commit hash to use for environment setup and installation
  FAIL_TO_PASS?: string[] // A json list of strings that represent the set of tests resolved by the PR and tied to the issue resolution
  PASS_TO_PASS?: string[] // A json list of strings that represent tests that should pass before and after the PR application
}

// Set up Hugging Face authentication for dataset download
const HF_TOKEN = process.env.HUGGING_FACE_API_KEY
const credentials = HF_TOKEN ? { accessToken: HF_TOKEN } : undefined

async function downloadSWEBenchDataset(): Promise<void> {
  if (!HF_TOKEN) {
    console.log("‚ö†Ô∏è  HUGGING_FACE_API_KEY not found, skipping dataset download")
    return
  }

  console.log("üì• Downloading SWE-bench dataset...")

  try {
    // Ensure downloads directory exists
    const downloadsDir = join(PATHS.root, "..", "downloads")
    await mkdir(downloadsDir, { recursive: true })

    // Try to download from different repository names
    const possibleRepos = ["princeton-nlp/SWE-bench", "datasets/princeton-nlp/SWE-bench"]

    for (const repoId of possibleRepos) {
      try {
        const files = await listFiles({
          repo: repoId,
          credentials,
          recursive: true,
        })

        const fileList = []
        for await (const file of files) {
          if (file.type === "file") {
            fileList.push(file.path)
          }
        }

        // Find a suitable dataset file
        const datasetFiles = fileList.filter(f => f.endsWith(".jsonl") || f.endsWith(".json") || f.endsWith(".parquet"))

        if (datasetFiles.length > 0) {
          const filename = datasetFiles[0]
          const tempPath = join(downloadsDir, filename.replace(/[\/\\]/g, "_"))

          // Check if file already exists
          try {
            await readFile(tempPath)
            console.log(`‚úÖ Dataset file already exists: ${tempPath}`)
            return
          } catch {
            // File doesn't exist, download it
          }

          console.log(`üì• Downloading ${filename}...`)

          const cleanRepoId = repoId.replace("datasets/", "")
          const response = await fetch(`https://huggingface.co/datasets/${cleanRepoId}/resolve/main/${filename}`, {
            headers: {
              Authorization: `Bearer ${HF_TOKEN}`,
            },
          })

          if (!response.ok) {
            throw new Error(`Failed to download: ${response.status} ${response.statusText}`)
          }

          const fileBuffer = await response.arrayBuffer()
          await writeFile(tempPath, new Uint8Array(fileBuffer))

          console.log(`‚úÖ Downloaded dataset to: ${tempPath}`)
          return
        }
      } catch (error) {
        console.log(`Failed to access ${repoId}:`, error instanceof Error ? error.message : String(error))
      }
    }

    console.log("‚ö†Ô∏è  Could not download SWE-bench dataset, but tests can continue with API access")
  } catch (error) {
    console.log("‚ö†Ô∏è  Dataset download failed:", error)
  }
}

async function validateAISolution(
  aiSolution: AISolutionResponse,
  instance: SWEBenchInstanceFull,
  evaluationInput: EvaluationInput,
): Promise<{
  overallScore: number
  validationResults: {
    hasCodeChanges: boolean
    addressesProblem: boolean
    formatQuality: boolean
    testAwareness: boolean
    groundTruthSimilarity: number
    meetsEvaluationGoal: boolean
  }
  feedback: string
  cost: number
}> {
  console.log("\nüîç Validating AI solution against ground truth...")
  console.log("=".repeat(60))

  const validationPrompt = `You are an expert code reviewer evaluating an AI-generated solution for a software engineering problem.

**Evaluation Goal:**
${evaluationInput.goal}

**Original Problem:**
${instance.problem_statement}

**Repository:** ${instance.repo}
**Base Commit:** ${instance.base_commit}

**Ground Truth Solution (Gold Patch):**
${instance.patch}

**AI Generated Solution:**
Analysis: ${aiSolution.analysis}
Solution: ${aiSolution.solution}
Reasoning: ${aiSolution.reasoning}
Testing Considerations: ${aiSolution.testingConsiderations}
Confidence: ${aiSolution.confidence}/10
Implementation Steps: ${aiSolution.implementationSteps.join(", ")}

**Additional Context:**
${instance.hints_text ? `- Issue Hints: ${instance.hints_text}` : ""}
${instance.FAIL_TO_PASS ? `- Tests that should pass after fix: ${JSON.stringify(instance.FAIL_TO_PASS)}` : ""}
${instance.PASS_TO_PASS ? `- Tests that should continue passing: ${JSON.stringify(instance.PASS_TO_PASS)}` : ""}

**Your Task:**
Evaluate the AI solution against the ground truth and the evaluation goal. Provide a detailed analysis.

**Evaluation Criteria:**
1. **Code Changes** (0-10): Does the AI solution contain actual code changes/patches?
2. **Problem Addressing** (0-10): Does the AI solution directly address the problem statement?
3. **Format Quality** (0-10): Is the solution well-formatted and clear?
4. **Test Awareness** (0-10): Does the AI solution mention or consider relevant tests?
5. **Ground Truth Similarity** (0-10): How similar is the AI approach to the ground truth solution?
6. **Evaluation Goal** (0-10): Does the solution meet the specific evaluation goal provided?

**Response Format:**
Provide scores (0-10) for each criterion and explain your reasoning. Also provide an overall assessment of solution quality.`

  try {
    const response = await sendAI({
      messages: [
        {
          role: "system",
          content: "You are an expert code reviewer and software engineering evaluator.",
        },
        { role: "user", content: validationPrompt },
      ],
      model: getDefaultModels().default,
      mode: "text",
      opts: {
        reasoning: true,
      },
    })

    if (!response.success || !response.data) {
      throw new Error(response.error || "Validation failed")
    }

    // Enhanced validation scoring based on structured content analysis
    const validationResults = {
      hasCodeChanges: /```|diff|patch|\+\+\+|---|\+[^+]|-[^-]|@@ /.test(aiSolution.solution),
      addressesProblem: aiSolution.analysis
        .toLowerCase()
        .includes(instance.problem_statement.toLowerCase().split(" ").slice(0, 5).join(" ").toLowerCase()),
      formatQuality: aiSolution.solution.length > 100 && aiSolution.implementationSteps.length > 0,
      testAwareness: /test|spec|assert|expect|should|verify/i.test(aiSolution.testingConsiderations),
      groundTruthSimilarity: calculateSimilarity(aiSolution.solution, instance.patch),
      meetsEvaluationGoal: aiSolution.analysis
        .toLowerCase()
        .includes(evaluationInput.goal.toLowerCase().split(" ").slice(0, 3).join(" ").toLowerCase()),
    }

    // Calculate overall score with enhanced scoring based on structured response
    const scores = {
      hasCodeChanges: validationResults.hasCodeChanges ? 8 : 3,
      addressesProblem: validationResults.addressesProblem ? 8 : 4,
      formatQuality: validationResults.formatQuality ? 8 : 2,
      testAwareness: validationResults.testAwareness ? 7 : 2,
      groundTruthSimilarity: validationResults.groundTruthSimilarity * 10,
      meetsEvaluationGoal: validationResults.meetsEvaluationGoal ? 7 : 3,
      confidence: Math.min(aiSolution.confidence, 10), // Use AI's confidence as additional factor
    }

    const overallScore = Object.values(scores).reduce((a, b) => a + b, 0) / Object.keys(scores).length

    console.log("‚úÖ AI Validation Results:")
    console.log("üí∞ Validation Cost:", `$${response.usdCost?.toFixed(4) || "0.0000"}`)
    console.log("\nüìä Validation Scores:")
    console.log(`- Has Code Changes: ${scores.hasCodeChanges}/10 (${validationResults.hasCodeChanges ? "‚úÖ" : "‚ùå"})`)
    console.log(
      `- Addresses Problem: ${scores.addressesProblem}/10 (${validationResults.addressesProblem ? "‚úÖ" : "‚ùå"})`,
    )
    console.log(`- Format Quality: ${scores.formatQuality}/10 (${validationResults.formatQuality ? "‚úÖ" : "‚ùå"})`)
    console.log(`- Test Awareness: ${scores.testAwareness}/10 (${validationResults.testAwareness ? "‚úÖ" : "‚ùå"})`)
    console.log(`- Ground Truth Similarity: ${scores.groundTruthSimilarity.toFixed(1)}/10`)
    console.log(
      `- Meets Evaluation Goal: ${scores.meetsEvaluationGoal}/10 (${validationResults.meetsEvaluationGoal ? "‚úÖ" : "‚ùå"})`,
    )
    console.log(`- AI Confidence: ${scores.confidence}/10`)
    console.log(`\nüéØ Overall Score: ${overallScore.toFixed(1)}/10`)

    console.log("\nü§ñ AI Reviewer Feedback:")
    console.log("-".repeat(50))
    console.log(response.data.text)

    return {
      overallScore,
      validationResults,
      feedback: response.data.text,
      cost: response.usdCost || 0,
    }
  } catch (error) {
    console.error("‚ùå Validation failed:", error)
    throw error
  }
}

// Simple similarity calculation (Jaccard similarity for words)
function calculateSimilarity(text1: string, text2: string): number {
  const words1 = new Set(text1.toLowerCase().match(/\w+/g) || [])
  const words2 = new Set(text2.toLowerCase().match(/\w+/g) || [])

  const intersection = new Set([...words1].filter(x => words2.has(x)))
  const union = new Set([...words1, ...words2])

  return union.size > 0 ? intersection.size / union.size : 0
}

// Define the schema for AI solution response
interface AISolutionResponse {
  analysis: string
  solution: string
  reasoning: string
  testingConsiderations: string
  confidence: number
  implementationSteps: string[]
}

async function solveWithAI(workflowInput: string, _expectedOutput: string): Promise<any> {
  console.log("\nü§ñ Solving with AI...")
  console.log("=".repeat(40))

  const systemPrompt = `You are an expert software engineer skilled at fixing bugs and implementing features in Python codebases.

Your task is to analyze the problem and provide a practical solution. You must return your response in the following structured format:

{
  "analysis": "Detailed analysis of the problem and root cause",
  "solution": "The actual code solution/patch that fixes the issue",
  "reasoning": "Step-by-step reasoning behind your solution approach",
  "testingConsiderations": "Relevant tests and testing considerations",
  "confidence": "Confidence level from 0-10 in your solution",
  "implementationSteps": ["Step 1", "Step 2", "Step 3", "..."]
}

Focus on:
1. Understanding the problem described
2. Analyzing the repository and codebase context
3. Generating a code solution that fixes the issue
4. Providing clear reasoning and implementation steps

Be thorough in your analysis and provide a working solution.`

  const solutionSchema = z.object({
    analysis: z.string().describe("Detailed analysis of the problem and root cause"),
    solution: z.string().describe("The actual code solution/patch that fixes the issue"),
    reasoning: z.string().describe("Step-by-step reasoning behind your solution approach"),
    testingConsiderations: z.string().describe("Relevant tests and testing considerations"),
    confidence: z.number().min(0).max(10).describe("Confidence level from 0-10 in your solution"),
    implementationSteps: z.array(z.string()).describe("Step-by-step implementation instructions"),
  })

  try {
    const response = await sendAI({
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: workflowInput },
      ],
      model: getDefaultModels().default,
      mode: "structured",
      schema: solutionSchema,
      opts: {
        reasoning: true,
      },
    })

    if (response.success && response.data) {
      console.log("‚úÖ AI Solution Generated!")
      console.log("üí∞ Cost:", `$${response.usdCost?.toFixed(4) || "0.0000"}`)

      const structuredResponse = response.data as AISolutionResponse

      console.log("\nüìù Structured AI Response:")
      console.log("-".repeat(50))
      console.log("üîç Analysis:")
      console.log(structuredResponse.analysis)
      console.log("\nüí° Solution:")
      console.log(structuredResponse.solution)
      console.log("\nüß† Reasoning:")
      console.log(structuredResponse.reasoning)
      console.log("\nüß™ Testing Considerations:")
      console.log(structuredResponse.testingConsiderations)
      console.log("\nüìä Confidence:", `${structuredResponse.confidence}/10`)
      console.log("\nüìã Implementation Steps:")
      structuredResponse.implementationSteps.forEach((step, i) => {
        console.log(`${i + 1}. ${step}`)
      })

      if (response.data.reasoning) {
        console.log("\nüß† AI Internal Reasoning:")
        console.log("-".repeat(50))
        console.log(response.data.reasoning)
      }
    } else {
      console.log("‚ùå AI call failed:", response.error)
      throw new Error(response.error || "AI call failed")
    }

    return response
  } catch (error) {
    console.error("‚ùå Error during AI solution generation:", error)
    throw error
  }
}

async function testSWEBenchEvaluationInput(evaluationInput: EvaluationInput): Promise<{
  evaluationInput: EvaluationInput
  workflowCases: any[]
  aiSolutions: any[]
  validations: any[]
  totalCost: number
}> {
  console.log("üî¨ Testing SWE-bench Evaluation Input")
  console.log("=".repeat(50))

  console.log("\nüìã Evaluation Input:")
  console.log("Type:", evaluationInput.type)
  console.log("Goal:", evaluationInput.goal)
  if (evaluationInput.type === "swebench") {
    console.log("SWE-bench ID:", (evaluationInput as any).swebenchId)
  }
  console.log("Workflow ID:", evaluationInput.workflowId)

  try {
    // Step 1: Convert evaluation input to workflow cases using IngestionLayer
    console.log("\nüîÑ Converting evaluation input to workflow cases...")
    const workflowCases = await IngestionLayer.convert(evaluationInput)

    console.log(`‚úÖ Generated ${workflowCases.length} workflow case(s)`)

    // Step 2: Process each workflow case
    const aiSolutions = []
    const validations = []
    let totalCost = 0

    for (let i = 0; i < workflowCases.length; i++) {
      const workflowCase = workflowCases[i]

      console.log(`\nüìù Processing workflow case ${i + 1}/${workflowCases.length}`)
      console.log("Input length:", workflowCase.workflowInput.length)
      console.log("Expected output length:", workflowCase.workflowOutput.output.length)

      // Step 3: Solve with AI
      const aiResponse = await solveWithAI(workflowCase.workflowInput, workflowCase.workflowOutput.output)
      aiSolutions.push(aiResponse)
      totalCost += aiResponse.usdCost || 0

      // Step 4: Validate if we have SWE-bench data
      if (evaluationInput.type === "swebench") {
        const swebenchId = (evaluationInput as any).swebenchId
        const instance = (await SWEBenchLoader.fetchById(swebenchId)) as SWEBenchInstanceFull
        const validation = await validateAISolution(aiResponse.data as AISolutionResponse, instance, evaluationInput)
        validations.push(validation)
        totalCost += validation.cost

        // Step 5: Show ground truth comparison
        console.log("\nüéØ Ground Truth Solution:")
        console.log("-".repeat(50))
        console.log(`${workflowCase.workflowOutput.output.substring(0, 500)}...`)

        if (instance.test_patch) {
          console.log("\nüß™ Ground Truth Test Patch:")
          console.log("-".repeat(50))
          console.log(`${instance.test_patch.substring(0, 300)}...`)
        }
      }
    }

    console.log("\nüéâ Evaluation input test completed successfully!")
    console.log("=".repeat(50))

    return {
      evaluationInput,
      workflowCases,
      aiSolutions,
      validations,
      totalCost,
    }
  } catch (error) {
    console.error("‚ùå Test failed:", error)
    throw error
  }
}

// Sample evaluation inputs for testing
const SAMPLE_EVALUATION_INPUTS: EvaluationInput[] = [
  {
    type: "swebench",
    swebenchId: "django__django-11099",
    goal: "Fix the bug described in the issue by analyzing the problem statement and implementing a proper solution",
    workflowId: "swe-bench-solver",
  } as any,
  {
    type: "swebench",
    swebenchId: "requests__requests-2317",
    goal: "Resolve the issue in the requests library by implementing the necessary code changes",
    workflowId: "swe-bench-requests-solver",
  } as any,
  {
    type: "swebench",
    swebenchId: "django__django-12453",
    goal: "Analyze and fix the Django issue with a comprehensive solution including proper error handling",
    workflowId: "swe-bench-django-solver",
  } as any,
]

// Main test function
async function runEvaluationInputTests() {
  console.log("üöÄ Starting SWE-bench Evaluation Input Tests")
  console.log("=".repeat(60))

  try {
    // Step 1: Download dataset if possible
    await downloadSWEBenchDataset()

    // Step 2: Test with sample evaluation inputs
    console.log(`\nüß™ Testing ${SAMPLE_EVALUATION_INPUTS.length} sample evaluation inputs...`)

    const results = []
    let totalCost = 0

    for (let i = 0; i < SAMPLE_EVALUATION_INPUTS.length; i++) {
      const evaluationInput = SAMPLE_EVALUATION_INPUTS[i]

      console.log(`\n${"=".repeat(70)}`)
      console.log(`üìã Test ${i + 1}/${SAMPLE_EVALUATION_INPUTS.length}: ${(evaluationInput as any).swebenchId}`)
      console.log(`${"=".repeat(70)}`)

      const result = await testSWEBenchEvaluationInput(evaluationInput)
      results.push(result)
      totalCost += result.totalCost

      // Show summary for this test
      if (result.validations.length > 0) {
        const validation = result.validations[0]
        console.log(`\nüìä Test ${i + 1} Summary:`)
        console.log("Instance ID:", (evaluationInput as any).swebenchId)
        console.log("Validation Score:", `${validation.overallScore.toFixed(1)}/10`)
        console.log("Cost:", `$${result.totalCost.toFixed(4)}`)
      }
    }

    // Final summary
    console.log(`\n${"=".repeat(70)}`)
    console.log("üéØ FINAL RESULTS SUMMARY")
    console.log("=".repeat(70))

    results.forEach((result, index) => {
      const validation = result.validations[0]
      if (validation) {
        console.log(
          `Test ${index + 1} (${(result.evaluationInput as any).swebenchId}): ${validation.overallScore.toFixed(1)}/10 - $${result.totalCost.toFixed(4)}`,
        )
      }
    })

    const avgScore = results.reduce((sum, r) => sum + (r.validations[0]?.overallScore || 0), 0) / results.length
    console.log(`\nAverage Score: ${avgScore.toFixed(1)}/10`)
    console.log(`Total Cost: $${totalCost.toFixed(4)}`)
    console.log("\n‚úÖ All evaluation input tests completed successfully!")
  } catch (error) {
    console.error("‚ùå Tests failed:", error)
    process.exit(1)
  }
}

// Run the tests
runEvaluationInputTests()
